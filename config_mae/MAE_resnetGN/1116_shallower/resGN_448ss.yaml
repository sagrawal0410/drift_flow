# Classification config for LatentResNet

wandb:
  project: "vae-test"
  entity: "mingyangd"
  name: null
  log_every_k: 40

# Dataset configuration
dataset:
  # Supported: mnist, cifar10, imagenet32, imagenet64, imagenet128, imagenet256
  name: "imagenet256_latent"
  img_shape: [4, 32, 32]
  tar_local: true

# Model configuration for LatentResNet
# Only simple fields are needed; `arch` is mapped to (block, layers) in code
model:
  num_classes: 1000
  in_channels: 4
  base_channels: 448
  layers: [2, 2, 2, 2]
  dropout_prob: 0.0
  patch_size: 2

# Training parameters
train:
  n_steps: 200000
  total_batch_size: 8192
  ema_dict:
    halflife_kimg: 4000
  finetune_save_per_step: 1000
  finetune_last_steps: 10000
  finetune_cls: 0.2
  compile_model: false
  use_bf16: true
  save_per_step: 10000
  eval_per_step: 1000
  eval_gen_batch_size: 256
  lr_schedule:
    lr: 0.001
    warmup_kimg: 10000
    total_kimg: 100000000
    clip_grad: 1.0
  load_dict:
    run_id: ""
    epoch: ""
    continue_training: false
  lambda_cls: 0.0
  mask_ratio_min: 0.5
  mask_ratio_max: 0.5


# Optimizer settings
optimizer:
  lr: 0.0001
  weight_decay: 0.01

model_type: "mae_resnet_gn"