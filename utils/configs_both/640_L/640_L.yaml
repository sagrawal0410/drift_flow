# Configs for Dit-based VAE training
# Path settings
wandb:
  project: "vae-test"
  entity: "mingyangd"
  name: null
  log_every_k: 40

mae:
  dataset:
    name: "imagenet256_latent"
    test_batch_size: 128
    img_shape: [4, 32, 32]
    tar_local: true
  model_type: "mae_resnet_gn"
  model:
    num_classes: 1000
    in_channels: 4
    base_channels: 544
    layers: [3,8,12,5]
    resnext_style: false
    dropout_prob: 0.0
    patch_size: 2
  train:
    n_steps: 200000
    total_batch_size: 8192
    ema_dict:
      halflife_kimg: 4000
    finetune_save_per_step: 1000
    finetune_last_steps: 4000
    finetune_cls: 0.2
    compile_model: false
    use_bf16: true
    save_per_step: 10000
    eval_per_step: 1000
    eval_gen_batch_size: 256
    lr_schedule:
      lr: 0.001
      warmup_kimg: 10000
      total_kimg: 100000000
      clip_grad: 1.0
    load_dict:
      run_id: ""
      epoch: ""
      continue_training: false
    lambda_cls: 0.0
    mask_ratio_min: 0.5
    mask_ratio_max: 0.5


  # Optimizer settings
  optimizer:
    lr: 0.0001
    weight_decay: 0.01

vae:
  use_cls: false
  
  dataset:
    name: "imagenet256_cache"
    test_batch_size: 128
    img_shape: [4, 32, 32]
    tar_local: true

  model:
    input_shape: [4, 32, 32]
    compile_mode: "default"
    decoder_config:
      input_size: 32
      patch_size: 2
      in_channels: 4
      hidden_size: 1024
      depth: 24
      num_heads: 16
      mlp_ratio: 4.0
      out_channels: 4
      cond_dim: 1024
      use_qknorm: true
      attn_drop: 0.0
      proj_drop: 0.0
      use_checkpoint: false
      use_rope: true
      use_swiglu: true
      use_rmsnorm: true
      wo_shift: false
      num_classes: 1000
      noise_classes: 64
      noise_coords: 16
      n_cls_tokens: 32
    loss_on_patches: []
    clip_dict:
      extract_kwargs:
        patch_group_size: 2
        patch_mean_size: [2,4]
        patch_std_size: [2,4]
        transpose_goal_dims: []
        min_res: 32
        use_mean: true
        use_std: true
        use_layers: [2,3,4]
        every_k_block: 2
    enable_clip: true
    feature_params:
      laplacian_load_dict:
        enable: false

  # Training parameters
  train:
    n_steps: 100000
    total_batch_size: 1024
    recon_per_step: 1000
    save_per_step: 2000
    eval_gen_per_step: 1000
    eval_recon_per_step: 1000
    compile_model: false
    use_bf16: true
    eval_bsz_per_gpu: 64
    lr_schedule:
      lr: 0.0004
      warmup_kimg: 10000
      total_kimg: 2000000
      clip_grad: 1.0
    augmentation_kwargs:
      enable: true
      augmentation_dim: 9
      kwargs: {}
    drop_class: 0.1
    load_dict:
      run_id: ""
      epoch: ""
      continue_training: false
    forward_dict:
      recon: 16
      attn_dict:
        kernel_type: "attn_new"
        R_list: [0.2]
        transpose_aff: true

    cond_mem_bank_size: 128   # capacity for conditional memory bank
    uncond_mem_bank_size: 1000 # capacity for unconditional memory bank
    gen_mem_bank_size: 0    # capacity for generator memory bank
    n_cond_samples: 32         # per-datapoint draws from conditional bank
    n_uncond_samples: 16        # per-datapoint draws from uncond bank
    n_gen_bank_samples: 0     # per-datapoint draws from gen bank (negatives
    min_cfg_scale: 1.0
    max_cfg_scale: 2.0
    neg_cfg_sampler_pow: 0.0
    eval_base_cfg: 2.0
    eval_cfg_list: [1.1, 1.2, 1.3, 1.4, 1.8, 2.0]
    n_classes: 1000
    push_warmup_steps: 100000 # number of steps to warmup the push
    warmup_push_size: 128 # number of pushed samples during warmup
    

  # Optimizer settings
  optimizer:
    lr: 0.0002
    weight_decay: 0.01 
    beta1: 0.9
    beta2: 0.95